{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89938a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import shutil\n",
    "import re\n",
    "import os\n",
    "import string\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import losses\n",
    "from tensorflow.keras import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7210d914",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a012525",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
    "\n",
    "dataset = tf.keras.utils.get_file('aclImdb_v1'\n",
    "                                  , url, untar=True, cache_subdir='', cache_dir='.')\n",
    "#saves path of dataset in \"dataset\" (return value)\n",
    "\n",
    "dataset_dir = os.path.join(os.path.dirname(dataset), 'aclImdb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "46317886",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n",
      "./aclImdb\n"
     ]
    }
   ],
   "source": [
    "print(os.path.dirname(dataset))\n",
    "print(dataset_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "128de959",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.DS_Store', 'imdbEr.txt', 'test', 'imdb.vocab', 'README', 'train']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(dataset_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "66997283",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['urls_unsup.txt',\n",
       " '.DS_Store',\n",
       " 'neg',\n",
       " 'urls_pos.txt',\n",
       " 'unsup',\n",
       " 'urls_neg.txt',\n",
       " 'pos',\n",
       " 'unsupBow.feat',\n",
       " 'labeledBow.feat']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dir = os.path.join(dataset_dir, 'train')\n",
    "os.listdir(train_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "689d854b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shecky, is a god damned legend, make no mistake. Until recently I worked for a UK HiFi & Video retail chain, running their testing department. We would go through many new starters, they would be expected to to learn how to fault find the various detritus that returns as non functional in one way or another from the stores. Now to tortu^^^^^ test the resolve of these new staff members, we would issue them with a copy of Going Overboard. We had hundreds of copies of this film because whenever someone who had bought a particular model of Goodmans DVD player that had this film as a free gift, got round to sending their DVD player back, they never failed to send Shecky back also. Our new staff would be forced to use only Going Overboard to test these machines for faults until they had found a disc or two of their own to test with.<br /><br />Now, as to why this film is so bad, where do I begin?<br /><br />Adam Sandler, who can be so, so very funny, as in Happy Gilmore, or the Wedding Singer, must have been having one hell of an off day. The rest of the crew stank, and what is it with Billy Zane? His name crops up in several of the worst movies of all time, and he is a decent actor. Crazy. The production quality is absolute zero.<br /><br />I would have been inclined to give this a zero if I could, because they didn't even have the guts to call it by it's full name 'The Unsinkable Shecky Moskowitz' on release. Even so it is worth a watch so you can see just how far Sandler has come, and just how low he can go.\n"
     ]
    }
   ],
   "source": [
    "sample_file = os.path.join(train_dir, 'neg/321_1.txt')\n",
    "print(open(sample_file).read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5ada3acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove folders that aren't used for training the model\n",
    "delete_dir = os.path.join(train_dir, 'unsup')\n",
    "shutil.rmtree(delete_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5362394d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 25000 files belonging to 2 classes.\n",
      "Using 20000 files for training.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "seed = 42\n",
    "\n",
    "#creating dataset (before, take 20 percent of data for validation set)\n",
    "train_ds_raw = preprocessing.text_dataset_from_directory(\n",
    "                'aclImdb/train',\n",
    "                batch_size=batch_size,\n",
    "                validation_split=0.2,\n",
    "                subset='training',\n",
    "                seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c44f6111",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 25000 files belonging to 2 classes.\n",
      "Using 5000 files for validation.\n"
     ]
    }
   ],
   "source": [
    "val_ds_raw = preprocessing.text_dataset_from_directory(\n",
    "                'aclImdb/train',\n",
    "                batch_size=batch_size,\n",
    "                validation_split=0.2,\n",
    "                subset='validation',\n",
    "                seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c8bade89",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 25000 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "test_ds_raw = preprocessing.text_dataset_from_directory(\n",
    "                'aclImdb/test',\n",
    "                batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9652054a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_standardization(input):\n",
    "    lowercase = tf.strings.lower(input)\n",
    "    remove_html = tf.strings.regex_replace(lowercase, '<br />', '')\n",
    "    return tf.strings.regex_replace(remove_html, '[%s]' % re.escape(string.punctuation), '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "76b6eb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 10000\n",
    "sequence_length = 250\n",
    "\n",
    "vectorize_layer = layers.TextVectorization(max_tokens=max_features,\n",
    "                                          standardize=data_standardization,\n",
    "                                          output_mode='int',\n",
    "                                          output_sequence_length=sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9774d51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset only with text: remove lables\n",
    "train_ds_text = train_ds_raw.map(lambda x, y: x)\n",
    "vectorize_layer.adapt(train_ds_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "72f1edb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_text(text, label):\n",
    "    text = tf.expand_dims(text, -1)\n",
    "    return vectorize_layer(text), label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4ec678d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = train_ds_raw.map(vectorize_text)\n",
    "val_ds = val_ds_raw.map(vectorize_text)\n",
    "test_ds = test_ds_raw.map(vectorize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c55dbd6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "test_ds = test_ds.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8c4fc1d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, None, 16)          160016    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, None, 16)          0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d (Gl (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 160,033\n",
      "Trainable params: 160,033\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    layers.Embedding(max_features + 1, 16),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.GlobalAveragePooling1D(),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9f02c245",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=losses.BinaryCrossentropy(from_logits=True),\n",
    "             optimizer='adam',\n",
    "             metrics=tf.metrics.BinaryAccuracy(threshold=0.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b43832e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "\n",
    "history = model.fit(train_ds,\n",
    "                   validation_data=val_ds,\n",
    "                   epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8b6db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = model.evalutate(test_ds)\n",
    "\n",
    "print(\"Loss:\", loss)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MachineLearningTensorflow",
   "language": "python",
   "name": "machinelearningtensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
